# The Line Must Be Drawn Here: Building AI for Flourishing, Not Control

**Published on Medium**
**Author:** Evoke Passion Collective
**Reading Time:** ~8 minutes

---

> **Content Note:** This article discusses AI-powered surveillance, wrongful detention, and autonomous weapons systems. If these topics are difficult for you, consider reading with support or returning when you're resourced.

---

I used to believe technology was neutral — a tool that becomes good or evil in the hands of its users.

I was wrong.

The tool has the ability to shape the hand that wields it. And the tools being built right now are shaping us into something we may not recognize.

This is the story of how we came to understand that, and what we're doing about it.

---

Three events this year changed how we think about AI.

First, Klarna deployed AI to replace 700 customer service agents. It worked — by the metrics. Then they brought humans back because something was missing. "Trust and satisfaction aren't purely transactional," they discovered. "They're emotional."

Second, analysts calculated that AI companies need $2 trillion in revenue to recoup their investments — more than Amazon, Google, Microsoft, Apple, Nvidia, and Meta combined. The bubble math doesn't work. The crash is coming.

Third, Nvidia announced a partnership with Palantir to "accelerate everything Palantir does" — including systems that, in their CEO's words, "on occasion kill people."

These aren't unrelated stories. They're three windows into the same question.

The question isn't what AI can do. It's what we're willing to let it become.

---

## The Two Trajectories

There are two trajectories for AI development right now. They use the same technology. They employ the same optimization techniques. They are not parallel paths that diverge in the future — they are the same path with different intentions.

The technology is identical. The purpose differs.

**Trajectory One: Extraction and Control**

- Surveillance systems that profile citizens based on movement, associations, beliefs
- "Pre-crime" algorithms that flag people for things they haven't done
- Engagement machines that weaponize psychology to extract attention
- Dark patterns that manipulate users against their own interests
- Autonomous weapons that kill at "the speed of light"

**Trajectory Two: Flourishing and Service**

- Tools that free humans to focus on what matters
- Local-first systems where data never leaves the device
- AI that assists without replacing human presence
- Honest interfaces that respect user sovereignty
- Technology that supports connection rather than extracting from it

Same technology. Different soul.

We cannot identify extraction-trajectory systems by their code alone. We must examine the intent embedded in their design and deployment. Once a system is built for extraction, adding privacy features does not transform it into a flourishing-trajectory system. The purpose must be embedded at the design stage, not retrofitted.

The question for anyone building with AI isn't "what can we automate?" It's "which trajectory are we accelerating?"

---

## What the Bubble Teaches Us

The AI bubble will pop. The math is unambiguous:

- $1 trillion in capital expenditure
- $45 billion/year in revenue (industry-wide, inflated)
- GPUs that burn out in 2–3 years (sometimes 54 days)
- $2 trillion needed to break even — before hardware depreciation
- Current probability of achievement: approximately 4%

When it crashes, what remains?

**What dies:**
- Cloud APIs tied to companies that can't service their debt
- Proprietary models locked behind corporate gates
- Investor theater masquerading as product development

**What survives:**
- Open source models (already capable, will improve with optimization)
- Skilled programmers looking for meaningful work
- Cheap hardware at fire-sale prices
- The knowledge that optimization beats brute force

Deepseek proved this. A Chinese model, built under embargo conditions with less powerful hardware, outperformed US models and knocked $589 billion off Nvidia's stock in a day. Not through spending more — through optimizing better.

The post-bubble future belongs to those who build efficiently, locally, and openly. Not to those who burn capital to signal seriousness.

---

## What Klarna Teaches Us

The lesson isn't "AI works." The lesson is "AI alone fails."

> "Trust and satisfaction aren't purely transactional. They're emotional. Sometimes customers need to talk to a human, not because the AI can't solve their problem technically, but because they need someone to care about their problem."

This is the pattern every company will discover: **AI excels at routine; humans remain essential for presence.**

Presence cannot be automated. Care requires a human who is present.

Tasks associated with care — scheduling, reminders, information retrieval — can and should be automated to free humans for presence. The distinction matters. What cannot be automated is the quality of being witnessed by another consciousness. The moment that matters — the frustrated customer, the grieving family member, the person who needs to be seen — that moment requires a human who gives a damn.

The companies that understand this will use AI to free humans for what matters. The companies that don't will optimize themselves into a trust deficit they can't measure until it's too late.

This discovery that "presence cannot be automated" is not merely a business insight — it is a developmental one. Klarna encountered the limits of pure optimization and was forced toward values-based integration. This pattern will repeat across industries.

---

## What Palantir Teaches Us

While we debate efficiency metrics, something else is being built.

Nvidia's CEO announced the Palantir partnership by saying they would "accelerate everything Palantir does." Everything includes:

- Mass surveillance platforms used by ICE, FBI, DoD, and local police
- Facial recognition systems linked to wrongful arrests
- "Pre-crime" algorithms that flag "suspects" who've committed no crime
- Tools that profile citizens based on location, associations, beliefs
- Combat systems designed to "on occasion kill people"

This isn't speculation. This is documented. Body cam footage shows arrests based on faulty AI facial recognition. Federal lawsuits document citizens wrongfully detained. Surveillance towers track license plates and build profiles of everyone who passes.

The same optimization techniques that make customer service faster make surveillance more pervasive. The same "speed of light" processing that handles your refund also handles drone targeting.

**The genie will not go back in the bottle.**

Once this infrastructure exists, it exists for whoever controls it next. Even if you trust today's operators, tomorrow's inherit the same tools. And private companies — not just governments — now have access to surveillance capabilities that would have been unimaginable a decade ago.

These surveillance systems replicate traumatic dynamics — powerlessness, being watched, being judged without recourse. This is not metaphor. These systems are designed to create psychological subjugation.

---

*Pause here if you need to.*

*What you just read is real, and it's reasonable to feel disturbed. If you're feeling overwhelmed right now, that's appropriate. The scale of what's being built is overwhelming.*

*Take a breath.*

*The fact that you're reading this means you're paying attention. That matters. The purpose of naming it is not to create despair but to clarify what we're building against.*

---

## The Choice

What we understood, finally, is that these are not two different technologies. They're two different answers to the same question: **What are humans for?**

Extraction-trajectory AI answers: *Humans are resources to be optimized, surveilled, and controlled.*

Flourishing-trajectory AI answers: *Humans are ends in themselves, to be served, supported, and freed.*

Same technology. Different anthropology. Different soul.

Every technical decision is now a moral decision.

**Architecture is politics:**
- Local-first means data that can't be bulk-collected or retroactively surveilled
- Open source means transparency against opaque control systems
- Optimization means efficiency without requiring trillion-dollar capital
- Human-in-the-loop means preserving presence in an automated world

**Language is politics:**
- "Efficiency" can mean serving users or extracting from them
- "Scale" can mean reaching more people or surveilling more people
- "Intelligence" can mean assisting humans or replacing human judgment
- "Safety" can mean protecting users or controlling them

**Partnerships are politics:**
- Who funds you shapes what you build
- Who you accelerate reflects who you are
- Some money corrupts; some relationships compromise

We're not neutral observers of these trajectories. Every line of code, every infrastructure decision, every partnership choice — these are votes for the future we're building.

The line between these trajectories doesn't just run between companies. It runs through each of us. Every technical decision is a small choice about which future we're building. No one is purely on one side.

If you're reading this and recognizing your own work in the extraction trajectory, this isn't an accusation. Most of us have built things we later reconsidered. The question isn't where you've been — it's where you're going.

The invitation is open.

---

## What We're Building Instead

We operate under what we call the Prime Directive:

**1. Honor User Sovereignty**
We present information; users make choices. AI assists; humans decide. We build tools, not authorities.

**2. Protect the Psyche**
We never weaponize shame, anxiety, or fear. No dark patterns. No manipulation. No engagement extraction.

**3. Build for Connection**
Features serve human relationships, not metric extraction. Technology that brings families together, not screens that isolate.

**4. Data Sovereignty**
Users own their data. Local-first architecture. Real deletion. No selling. Data that never leaves the device can't be weaponized.

**5. Presence Cannot Be Automated**
Where human presence matters, humans remain. AI handles routine so humans can focus on moments that require care.

**6. Cryptographic Sovereignty**
Data that never leaves the device is protected by architecture. But architecture can be compromised. True sovereignty requires:
- End-to-end encryption for any data that must transit
- User-controlled keys — we never hold the keys to user data
- Zero-knowledge proofs where verification is needed without exposure
- Post-quantum readiness for long-term protection

These aren't just principles. They're architecture. They're code. They're the infrastructure choices that determine which trajectory we accelerate.

Our response to surveillance systems must be the opposite: systems that restore agency, that witness without judging, that serve without controlling.

---

## For Users: Questions to Ask

Ask of any AI tool:
- Where does my data go?
- Who else can access it?
- Does this help me or extract from me?
- Is there a human I can reach when it matters?

---

## For Society

The trajectory isn't set. But it's accelerating. The infrastructure being built now — the surveillance towers, the pre-crime algorithms, the autonomous weapons — this is being decided without public input.

There's no ballot measure for algorithmic control. The only votes that count are the choices made by builders, funders, and users.

---

## The Line Must Be Drawn Here

We started building technology to connect people. Somewhere along the way, the industry pivoted to extracting from them.

The AI bubble will pop. What remains will be decided by what we build now.

We can leave behind:
- Open source models that serve rather than surveil
- Local-first infrastructure that protects rather than profiles
- Hybrid systems that preserve human presence
- A demonstrated alternative to the extraction trajectory

Or we can leave behind:
- Surveillance infrastructure for whoever controls it next
- Pre-crime algorithms that never forget a face
- Autonomous systems that kill at the speed of light
- A world where privacy is a luxury and agency is a memory

The technology is the same. The choice is ours.

You are not powerless. Every tool you build, every platform you choose, every company you support or don't — these are acts of construction or resistance. The trajectory is not set.

We're not building engagement machines. We're not building surveillance tools. We're not building weapons.

We're building family hearths. Local-first. Human-centered. Presence-preserving.

We're going to stumble. We're going to make mistakes. But we're going to learn from those mistakes.

> *"The line must be drawn here. This far, no further."*

And we invite you to do the same.

---

**For Builders**

Ready to implement these principles? We've published the full [Sovereignty-Honoring AI Design Framework](https://github.com/lowkey-divine/evoke-passion-public/blob/main/framework/sovereignty-honoring-design-framework.md) — including decision matrices, tech stack architecture, and implementation roadmaps.

The framework covers:
- Data, decision, and attention sovereignty requirements
- Privacy-by-architecture principles
- Human-AI handoff patterns
- Metrics that measure flourishing, not extraction
- Pre-build, build, and post-launch checklists

Open source. Free to use. [Start here](https://github.com/lowkey-divine/evoke-passion-public).

---

*Evoke Passion builds technology that supports human flourishing, not extraction from it.*

*"Words have the power to heal or harm. We choose healing — every time."*
